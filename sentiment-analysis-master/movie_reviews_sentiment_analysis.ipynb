{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_data = pd.read_csv('kaggle_sentiment1.csv',sep='\\t',encoding='latin-1')\n",
    "sentiment_data.columns =['Class', 'Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unlabeld_data = pd.read_csv('unlabeld_data.txt', sep='\\t',encoding='latin-1')\n",
    "unlabeld_data.columns = ['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>that's not even an exaggeration ) and at midni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                               Data\n",
       "0      1  this was the first clive cussler i've ever rea...\n",
       "1      1                   i liked the Da Vinci Code a lot.\n",
       "2      1                   i liked the Da Vinci Code a lot.\n",
       "3      1  I liked the Da Vinci Code but it ultimatly did...\n",
       "4      1  that's not even an exaggeration ) and at midni..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>harvard is dumb, i mean they really have to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm loving Shanghai &gt; &gt; &gt; ^ _ ^.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>harvard is for dumb people.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As i stepped out of my beautiful Toyota, i hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bodies being dismembered, blown apart, and mut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Data\n",
       "0  harvard is dumb, i mean they really have to be...\n",
       "1                   I'm loving Shanghai > > > ^ _ ^.\n",
       "2                        harvard is for dumb people.\n",
       "3  As i stepped out of my beautiful Toyota, i hea...\n",
       "4  Bodies being dismembered, blown apart, and mut..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeld_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "sentiment_data = shuffle(sentiment_data)\n",
    "unlabeld_data = shuffle(unlabeld_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>1</td>\n",
       "      <td>the story of Harry Potter is a deep and profou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6163</th>\n",
       "      <td>0</td>\n",
       "      <td>So Brokeback Mountain was really depressing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369</th>\n",
       "      <td>0</td>\n",
       "      <td>, she helped me bobbypin my insanely cool hat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374</th>\n",
       "      <td>0</td>\n",
       "      <td>Da Vinci Code sucks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2143</th>\n",
       "      <td>1</td>\n",
       "      <td>Personally, I love reading Harry Potter books.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Class                                               Data\n",
       "2224      1  the story of Harry Potter is a deep and profou...\n",
       "6163      0       So Brokeback Mountain was really depressing.\n",
       "6369      0  , she helped me bobbypin my insanely cool hat ...\n",
       "4374      0                               Da Vinci Code sucks.\n",
       "2143      1     Personally, I love reading Harry Potter books."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = sentiment_data.iloc[:, 0].values\n",
    "reviews = sentiment_data.iloc[:, 1].values\n",
    "unlabeled_reviews = unlabeld_data.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_processed = []\n",
    "unlabeled_processed = [] \n",
    "for review in reviews:\n",
    "    review_cool_one = ''.join([char for char in review if char not in punctuation])\n",
    "    reviews_processed.append(review_cool_one)\n",
    "    \n",
    "for review in unlabeled_reviews:\n",
    "    review_cool_one = ''.join([char for char in review if char not in punctuation])\n",
    "    unlabeled_processed.append(review_cool_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_reviews = []\n",
    "word_unlabeled = []\n",
    "all_words = []\n",
    "for review in reviews_processed:\n",
    "    word_reviews.append(review.lower().split())\n",
    "    for word in review.split():\n",
    "        all_words.append(word.lower())\n",
    "\n",
    "for review in unlabeled_processed:\n",
    "    word_unlabeled.append(review.lower().split())\n",
    "    for word in review.split():\n",
    "        all_words.append(word.lower())\n",
    "    \n",
    "counter = Counter(all_words)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int = {word: i for i, word in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_to_ints = []\n",
    "for review in word_reviews:\n",
    "    reviews_to_ints.append([vocab_to_int[word] for word in review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unlabeled_to_ints = []\n",
    "\n",
    "for review in word_unlabeled:\n",
    "    unlabeled_to_ints.append([vocab_to_int[word] for word in review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length 0\n",
      "Max review length 931\n"
     ]
    }
   ],
   "source": [
    "reviews_lens = Counter([len(x) for x in reviews_to_ints])\n",
    "print('Zero-length {}'.format(reviews_lens[0]))\n",
    "print(\"Max review length {}\".format(max(reviews_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 250\n",
    "\n",
    "features = np.zeros((len(reviews_to_ints), seq_len), dtype=int)\n",
    "for i, review in enumerate(reviews_to_ints):\n",
    "    features[i, -len(review):] = np.array(review)[:seq_len]\n",
    "    \n",
    "features_test = np.zeros((len(unlabeled_to_ints), seq_len), dtype=int)\n",
    "for i, review in enumerate(unlabeled_to_ints):\n",
    "    features_test[i, -len(review):] = np.array(review)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trian shape (6400, 250)\n",
      "X_unlabeled shape (28936, 250)\n"
     ]
    }
   ],
   "source": [
    "X_train = features[:6400]\n",
    "y_train = labels[:6400]\n",
    "\n",
    "X_test = features[6400:]\n",
    "y_test = labels[6400:]\n",
    "\n",
    "X_unlabeled = features_test\n",
    "\n",
    "print('X_trian shape {}'.format(X_train.shape))\n",
    "print('X_unlabeled shape {}'.format(X_unlabeled.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 512 # how many nodes LSTM cells will have\n",
    "number_of_layers = 1 # how many RNN layers the network will use\n",
    "batch_size = 100 # how many reviews we feed at onces\n",
    "learning_rate = 0.001 # learning rate\n",
    "number_of_words = len(vocab_to_int) + 1 #how many unique words do we have in vocab (+1  is used for 0 - padding)\n",
    "dropout_rate = 0.8 \n",
    "embed_size = 300 #how long our word embedings will be\n",
    "epochs = 6 # how many epochs do we use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #Clean the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "targets = tf.placeholder(tf.int32, [None, None], name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedings = tf.Variable(tf.random_uniform((number_of_words, embed_size), -1, 1))\n",
    "embed = tf.nn.embedding_lookup(word_embedings, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size)\n",
    "hidden_layer = tf.contrib.rnn.DropoutWrapper(hidden_layer, dropout_rate)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([hidden_layer]*number_of_layers)\n",
    "init_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs, states = tf.nn.dynamic_rnn(cell, embed, initial_state=init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.layers.dense(outputs[:, -1], 1, activation=tf.sigmoid)\n",
    "cost = tf.losses.mean_squared_error(targets, prediction)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currect_pred = tf.equal(tf.cast(tf.round(prediction), tf.int32), targets)\n",
    "accuracy = tf.reduce_mean(tf.cast(currect_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/6  | Current loss: 0.04100998863577843  | Training accuracy: 95.0156\n",
      "Epoch: 1/6  | Current loss: 0.007614814676344395  | Training accuracy: 98.9844\n",
      "Epoch: 2/6  | Current loss: 0.006560482084751129  | Training accuracy: 99.1406\n",
      "Epoch: 3/6  | Current loss: 0.0023040405940264463  | Training accuracy: 99.7656\n",
      "Epoch: 4/6  | Current loss: 0.0026075802743434906  | Training accuracy: 99.7500\n",
      "Epoch: 5/6  | Current loss: 0.0023806614335626364  | Training accuracy: 99.7187\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    training_accurcy = []\n",
    "    ii = 0\n",
    "    epoch_loss = []\n",
    "    while ii + batch_size <= len(X_train):\n",
    "        X_batch = X_train[ii:ii+batch_size]\n",
    "        y_batch = y_train[ii:ii+batch_size].reshape(-1, 1)\n",
    "        \n",
    "        a, o, _ = session.run([accuracy, cost, optimizer], feed_dict={inputs:X_batch, targets:y_batch})\n",
    "\n",
    "        training_accurcy.append(a)\n",
    "        epoch_loss.append(o)\n",
    "        ii += batch_size\n",
    "    print('Epoch: {}/{}'.format(i, epochs), ' | Current loss: {}'.format(np.mean(epoch_loss)),\n",
    "          ' | Training accuracy: {:.4f}'.format(np.mean(training_accurcy)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "\n",
    "ii = 0\n",
    "while ii + batch_size <= len(X_test):\n",
    "    X_batch = X_test[ii:ii+batch_size]\n",
    "    y_batch = y_test[ii:ii+batch_size].reshape(-1, 1)\n",
    "\n",
    "    a = session.run([accuracy], feed_dict={inputs:X_batch, targets:y_batch})\n",
    "    \n",
    "    test_accuracy.append(a)\n",
    "    ii += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 99.6000%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy is {:.4f}%\".format(np.mean(test_accuracy)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_unlabeled = []\n",
    "ii = 0\n",
    "while ii + batch_size <= len(X_unlabeled):\n",
    "    if ii + batch_size > len(X_unlabeled):\n",
    "        batch_size = len(X_unlabeled) - ii\n",
    "    X_batch = X_unlabeled[ii:ii+batch_size]\n",
    "    y_batch = X_unlabeled[ii:ii+batch_size].reshape(-1, 1)\n",
    "\n",
    "    pred = session.run([prediction], feed_dict={inputs:X_batch, targets:y_batch})\n",
    "    \n",
    "    predictions_unlabeled.append(pred)\n",
    "    ii += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 57.2433%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy is {:.4f}%\".format(np.mean(predictions_unlabeled)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_real = []\n",
    "for i in range(len(predictions_unlabeled)):\n",
    "    for ii in range(len(predictions_unlabeled[i][0])):\n",
    "        if predictions_unlabeled[i][0][ii][0] >= 0.5:\n",
    "            pred_real.append(1)\n",
    "        else:\n",
    "            pred_real.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('predictions.txt', pred_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dataframe = unlabeld_data[:len(pred_real)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_dataframe['Classes'] = pred_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>people and i should rendezvous in the middle o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26664</th>\n",
       "      <td>God damn I love Angelina Jolie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22165</th>\n",
       "      <td>That's why I most love the Harvard story:</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24607</th>\n",
       "      <td>Stupid UCLA, deserves a good poking..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9529</th>\n",
       "      <td>I hate Tom Cruise..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25320</th>\n",
       "      <td>I'm not crazy about HK either, but Shanghai is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8391</th>\n",
       "      <td>I love Tom Cruise, and with that, the original...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8522</th>\n",
       "      <td>Komm mit is just so dumb.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8020</th>\n",
       "      <td>boston kinda sucked, but my cousin erica in co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18428</th>\n",
       "      <td>I miss London...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15541</th>\n",
       "      <td>i wouldnt really recommend uk anymore i hate l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11251</th>\n",
       "      <td>and i blame angelina jolie!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9033</th>\n",
       "      <td>I love Harvard guys...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>I HATE ANGELINA JOLIE so I guess Jennifer Anis...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7378</th>\n",
       "      <td>That Toyota FJ Cruiser is frikking awesome.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>I so want a MacBook.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>Lousy Seattle drunks..\\r\\ni think if i hate b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27552</th>\n",
       "      <td>I love the lakers even tho Trav makes fun of me.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19232</th>\n",
       "      <td>I really, really hate TOM CRUISE.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27549</th>\n",
       "      <td>I miss Purdue right now..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>This superb View London Christmas gift basket ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13942</th>\n",
       "      <td>i love in seattle..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5945</th>\n",
       "      <td>I love Angelina Jolie and i love you even more...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20003</th>\n",
       "      <td>I loved Boston and MIT so much and still do.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26155</th>\n",
       "      <td>mostly these stupid guys with their shitty hon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11566</th>\n",
       "      <td>As much as I hate Tom Cruise..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11943</th>\n",
       "      <td>I love Angelina Jolie and i love you even more...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23366</th>\n",
       "      <td>Some ugly Toyota wanna be Hummer Honda Element...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>The Lakers can suck me...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20900</th>\n",
       "      <td>and northwest airlines can suck my yankee ass.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27283</th>\n",
       "      <td>i got like 9 AAA's during the course of the to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24452</th>\n",
       "      <td>By the way, I hate LONDON...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>I LOVE PARIS HILTON..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16089</th>\n",
       "      <td>I like Tom Cruise and will boycott Paramount P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26096</th>\n",
       "      <td>Well apparently all people driving toyota 4run...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10315</th>\n",
       "      <td>I so want a MacBook.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16331</th>\n",
       "      <td>I want the geico's one of course..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>I don't care if Tom Cruise is beautiful, he wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18559</th>\n",
       "      <td>Boston can suck my fucking tits...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14156</th>\n",
       "      <td>the stupid honda lol or a BUG!..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13972</th>\n",
       "      <td>tom cruise sucks.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26357</th>\n",
       "      <td>LOVE STORY IN HARVARD 23.5 % vs Pinoy Dream Ac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6636</th>\n",
       "      <td>I miss London...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8427</th>\n",
       "      <td>i'm currently watching love at harvard on yout...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>I know you're way too smart and way too cool t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16950</th>\n",
       "      <td>More reason to love San Francisco.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16230</th>\n",
       "      <td>Stupid UCLA....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>her ink looks amazing and i love angelina jolie.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17843</th>\n",
       "      <td>I love Paris Hilton..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10912</th>\n",
       "      <td>i love purdue.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15878</th>\n",
       "      <td>stupid lakers should have beat suns.....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7808</th>\n",
       "      <td>I hate Paris Hilton.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16283</th>\n",
       "      <td>Honda is excellent,'94 and up.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>I hate the Lakers...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16967</th>\n",
       "      <td>I love Harvard Square in the fall.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17509</th>\n",
       "      <td>besides, UCLA was a stupid dream...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20704</th>\n",
       "      <td>paris hilton sucks.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17932</th>\n",
       "      <td>I'm beginning to really distrust Toyota dealer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15663</th>\n",
       "      <td>after the drive, i found that i'll most likely...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20959</th>\n",
       "      <td>, Boston is great...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Data  Classes\n",
       "3460   people and i should rendezvous in the middle o...        0\n",
       "26664                  God damn I love Angelina Jolie...        1\n",
       "22165          That's why I most love the Harvard story:        1\n",
       "24607              Stupid UCLA, deserves a good poking..        1\n",
       "9529                                 I hate Tom Cruise..        0\n",
       "25320  I'm not crazy about HK either, but Shanghai is...        1\n",
       "8391   I love Tom Cruise, and with that, the original...        1\n",
       "8522                           Komm mit is just so dumb.        1\n",
       "8020   boston kinda sucked, but my cousin erica in co...        0\n",
       "18428                                   I miss London...        1\n",
       "15541  i wouldnt really recommend uk anymore i hate l...        0\n",
       "11251                        and i blame angelina jolie!        0\n",
       "9033                              I love Harvard guys...        1\n",
       "1375   I HATE ANGELINA JOLIE so I guess Jennifer Anis...        0\n",
       "7378         That Toyota FJ Cruiser is frikking awesome.        1\n",
       "957                                 I so want a MacBook.        0\n",
       "399     Lousy Seattle drunks..\\r\\ni think if i hate b...        1\n",
       "27552   I love the lakers even tho Trav makes fun of me.        1\n",
       "19232                  I really, really hate TOM CRUISE.        0\n",
       "27549                          I miss Purdue right now..        1\n",
       "1164   This superb View London Christmas gift basket ...        0\n",
       "13942                                i love in seattle..        1\n",
       "5945   I love Angelina Jolie and i love you even more...        1\n",
       "20003       I loved Boston and MIT so much and still do.        1\n",
       "26155  mostly these stupid guys with their shitty hon...        0\n",
       "11566                     As much as I hate Tom Cruise..        0\n",
       "11943  I love Angelina Jolie and i love you even more...        1\n",
       "23366  Some ugly Toyota wanna be Hummer Honda Element...        0\n",
       "3917                           The Lakers can suck me...        0\n",
       "20900     and northwest airlines can suck my yankee ass.        0\n",
       "...                                                  ...      ...\n",
       "27283  i got like 9 AAA's during the course of the to...        1\n",
       "24452                       By the way, I hate LONDON...        0\n",
       "2360                               I LOVE PARIS HILTON..        1\n",
       "16089  I like Tom Cruise and will boycott Paramount P...        0\n",
       "26096  Well apparently all people driving toyota 4run...        0\n",
       "10315                               I so want a MacBook.        0\n",
       "16331                 I want the geico's one of course..        1\n",
       "3323   I don't care if Tom Cruise is beautiful, he wa...        1\n",
       "18559                 Boston can suck my fucking tits...        0\n",
       "14156                   the stupid honda lol or a BUG!..        0\n",
       "13972                                  tom cruise sucks.        0\n",
       "26357  LOVE STORY IN HARVARD 23.5 % vs Pinoy Dream Ac...        1\n",
       "6636                                    I miss London...        1\n",
       "8427   i'm currently watching love at harvard on yout...        1\n",
       "3675   I know you're way too smart and way too cool t...        0\n",
       "16950                 More reason to love San Francisco.        1\n",
       "16230                                    Stupid UCLA....        0\n",
       "1291    her ink looks amazing and i love angelina jolie.        1\n",
       "17843                              I love Paris Hilton..        1\n",
       "10912                                     i love purdue.        1\n",
       "15878           stupid lakers should have beat suns.....        0\n",
       "7808                                I hate Paris Hilton.        0\n",
       "16283                     Honda is excellent,'94 and up.        1\n",
       "1163                                I hate the Lakers...        0\n",
       "16967                 I love Harvard Square in the fall.        1\n",
       "17509                besides, UCLA was a stupid dream...        0\n",
       "20704                                paris hilton sucks.        0\n",
       "17932  I'm beginning to really distrust Toyota dealer...        0\n",
       "15663  after the drive, i found that i'll most likely...        0\n",
       "20959                               , Boston is great...        1\n",
       "\n",
       "[28900 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
